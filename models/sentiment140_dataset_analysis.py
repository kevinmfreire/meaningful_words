# -*- coding: utf-8 -*-
"""sentiment140-dataset-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JbWq5O5WAyjiszlRAqtdt-8bYvGcJ88J

# Tweet Dataset Analysis

## First let's download datasets from google drive to google collab.
"""

from google.colab import drive
drive.mount('/gdrive')

!cp -r /gdrive/MyDrive/SharpestMinds/datasets/ /content/

"""##Now that we have our dataset, we have three objectives: 
* Understand the dataset and clean it up
* Build a classification model to predict the twitter sentiment
* Compare the evalutation metrics of a few classification algorithms

We first want to import our libraries
"""

!pip install scikit-plot

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

import re
import string
import nltk
import math

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split,RandomizedSearchCV,RepeatedStratifiedKFold,GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score,roc_auc_score, roc_curve, precision_score, recall_score
from scikitplot.metrics import plot_roc_curve as auc_roc

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import BernoulliNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier

# Dwnloading NLTK packages
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

"""## Data Exploration"""

# Load dataset then check columns and values
DATASET_COLUMNS  = ["sentiment", "ids", "date", "flag", "user", "text"]
DATASET_ENCODING = "ISO-8859-1"
tweet_df = pd.read_csv("./datasets/sentiment140/tweets.csv", encoding=DATASET_ENCODING , names=DATASET_COLUMNS)
tweet_df.head()

"""* Let's check additional information on dataset"""

tweet_df.describe(include='O')

"""* We do not need the date, ids, flag or user, I will drop those columns and keepy sentiment and text."""

# Dropping the twitter-tweets dataset
tweet_df.drop(['ids','date', 'flag', 'user'], axis=1, inplace=True)
tweet_df.head()

"""* The Kaggle Sentiment140 dataset has values 0=negative and 4=positive.
* I will replace all values with -1=negative and 1=positive.
"""

to_sentiment = {0: "negative", 4: "positive"}
tweet_df.sentiment = tweet_df.sentiment.apply(lambda x: to_sentiment[x])
tweet_df.head()

"""* I create a target value to select the column 'sentiment'
* Then I copy the dataset as original_df
"""

target = 'sentiment'
original_df = tweet_df.copy(deep=True)

print('\n\033[1mData Dimension:\033[0m Dataset consists of {} columns & {} records.'.format(tweet_df.shape[1], tweet_df.shape[0]))

# Let's check the dtypes of all columns

tweet_df.info()

# Checking the stats of all the columns

tweet_df.describe()

"""# Data Processing"""

# Check for empty elements

tweet_df.isnull().sum()

# Remove any missing values
# rom the above cell there are no missing values so we do not run this cell

tweet_df.dropna(inplace=True)
original_df = tweet_df.copy(deep=True)

tweet_df[tweet_df.duplicated()]

# Let's remove duplicated rows (if any)

counter = 0
r, c = original_df.shape

tweet_df_dedup = tweet_df.drop_duplicates()
tweet_df_dedup.reset_index(drop=True, inplace=True)

if tweet_df_dedup.shape==(r,c):
  print('\n\033[1mInference:\033[0m The dataset doesn\'t have any duplicates')
else:
  print(f'\n\033[1mInference:\033[0m Number of duplicates dropped/fixed ---> {r-tweet_df_dedup.shape[0]}')

tweet_df_dedup.head()

"""With computational complexity I will reduce the data by a scale of 4 so the text pre-processing will not take too long and once we have a baseline model I will then train the model with the entire dataset."""

def split_data(data, scale):
  neg_df = data[data['sentiment']=='negative']
  pos_df = data[data['sentiment']=='positive']
  neg_df = neg_df[0:(len(neg_df)//scale)]
  pos_df = pos_df[0:(len(pos_df)//scale)]
  new_df = [neg_df, pos_df]
  new_df = pd.concat(new_df)
  return new_df

new_df_split = split_data(tweet_df_dedup, 1000)
new_df_split.info()

"""## Let's do some basic text processing such as:
* Convert to lower case
* Tokenisation
* Remove puntuation
* Remove stop words
* Stemming
* Lemmatization
"""

# Cleaning the text
tweet_df_clean = new_df_split.copy()

def preprocessor(text):
  text = re.sub('[^a-zA-Z]',' ', text)    # remove punctuation
  text = text.lower()                     # convert to lowercase
  text = text.strip()                     # remove leading and tailing whitespaces
  # Stemming
  text = ''.join([i for i in text if i in string.ascii_lowercase+' '])
  text = ' '.join([word for word in text.split() if word.isalnum()])  
  text = ' '.join([WordNetLemmatizer().lemmatize(word,pos='v') for word in text.split()]) 
  text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
  return text

tweet_df_clean['text'] = new_df_split['text'].apply(preprocessor)
tweet_df_clean.head()

"""**Inference:** The text is now clean from the removal of all punctuations, stop words and stemming.

* We next want to tokenize our dataset using Porter Stemmer
"""

porter = PorterStemmer()

def tokenizer_porter(text):
  return [porter.stem(word) for word in text.split()]

"""Let's extract features using TF-IDF"""

tf_idf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, tokenizer=tokenizer_porter, use_idf=True, norm='l2', smooth_idf=True)
label=tweet_df_clean[target].values
features=tf_idf.fit_transform(tweet_df_clean.text)

"""Let's look at the labels"""

label

"""Now let's look at the features"""

features

"""## Exploratory Data Analysis (EDA)"""

# Let'sanalyze the distribution oof the target values
print('\033[1mTarget Variable Distribution'.center(55))
plt.pie(tweet_df_clean[target].value_counts(), labels=['Negative','Positive'], counterclock=False, shadow=True, 
        explode=[0,0.1], autopct='%1.1f%%', radius=1.5, startangle=0)
plt.show()

"""Both the Negative and Positive values seem to be about the same."""

# Visualizing the average text sequence length

tweet_df_rl = tweet_df_clean.copy()
tweet_df_rl['review_length'] = 0

tweet_df_rl['review_length'] = tweet_df_rl['text'].apply(lambda x:len(x))

plt.figure(figsize=[20,8])
sns.boxplot(x=label, y='review_length', data=tweet_df_rl, palette=['red', 'limegreen'])
plt.title('Text Sequence Length')
plt.xlabel("Sentiment")
plt.ylabel("Length of reviews")
plt.show()

"""**The sequence length for reviews on an average are almost similar for both positive and negative tweets**

* Let's compare with the original tweet dataframe we save earlier
"""

original_df = new_df_split.copy()
pos_df = original_df[original_df[target]=='positive']['text']
neg_df = original_df[original_df[target]=='negative']['text']

def freq_text(text_df):
  combi_text=""
  for x in text_df.values:
    combi_text+=' '.join(x.split())
  combi_text = [x for x in combi_text.split() if len(x) >3 and x not in stopwords.words('english')]
  return combi_text

def freq_df(text_count_df, senti):
  fredis=nltk.FreqDist(text_count_df)
  fredis_df = pd.DataFrame({senti: list(fredis.keys()), 'Count': list(fredis.values())})
  fredis_df = fredis_df.sort_values(by='Count', ascending=False)
  return fredis_df

def plot_freq_dis(text_count_df,senti):
  sns.barplot(data=text_count_df[:10], x=senti, y='Count')

pos_df.shape

pos_df = pos_df.apply(preprocessor)

senti = 'positive'
pos_text = freq_text(pos_df)
pos_freq_df = freq_df(pos_text,senti)
plot_freq_dis(pos_freq_df,senti)

neg_df.shape

neg_df = neg_df.apply(preprocessor)

senti='negative'
neg_text = freq_text(neg_df)
neg_freq_df = freq_df(neg_text, senti)
plot_freq_dis(neg_freq_df, senti)

def wordcloud_draw(data, color, s):
    words = ' '.join(data)
    cleaned_word = " ".join([word for word in words.split() if(word!='movie' and word!='film')])
    wordcloud = WordCloud(stopwords=stopwords.words('english'),background_color=color,width=2500,height=2000).generate(cleaned_word)
    plt.imshow(wordcloud)
    plt.title(s)
    plt.axis('off')

plt.figure(figsize=[20,10])
plt.subplot(1,2,1)
wordcloud_draw(pos_df,'coral','Frequently used postive words')
plt.subplot(1,2,2)
wordcloud_draw(neg_df, 'limegreen','Frequently used negative words')
plt.show()

"""# Predictive Modeling

Let's begin a simple training
"""

#  Assigning labels to target variable

label_mapping = {'negative':0, 'positive':1}
tweet_df_clean[target] = tweet_df_clean[target].map(label_mapping)

# Split data into training and testing sets

X = features
y = pd.Series(label).map(label_mapping)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print('Original set  ---> ','feature size: ',X.shape,'label size',len(y))
print('Training set  ---> ','feature size: ',X_train.shape,'label size',len(y_train))
print('Test set  --->  ','feature size: ',X_test.shape,'label size',len(y_test))

# Let's create a table to store the results of various models

result_df = pd.DataFrame(columns=['Models','Accuracy','Precision','Recall','F1-score','AUC-ROC score'])
result_df['Models']=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Naive Bayes Classifier (NB)']
result_df.fillna(0.0,inplace=True)
result_df.set_index('Models',inplace=True)
result_df

result_df.loc['Logistic Regression (LR)','Accuracy']

#Classification Summary Function
def classification_summary(pred,pred_prob,y_test, model):
  result_df.loc[model,'Accuracy']   =round(accuracy_score(y_test,pred),3)*100   
  result_df.loc[model,'Precision']=round(precision_score(y_test, pred, average='weighted'),3)*100 
  result_df.loc[model,'Recall']=round(recall_score(y_test, pred, average='weighted'),3)*100 
  result_df.loc[model,'F1-score']=round(f1_score(y_test, pred, average='weighted'),3)*100 
  # result_df.loc[model,'AUC-ROC score']=round(roc_auc_score(y_test, pred_prob, multi_class='ovr'),3)*100

  print('{}{}\033[1m Evaluating {} \033[0m{}{}\n'.format('<'*3,'-'*35,model, '-'*35,'>'*3))
  print('Accuracy = {}%'.format(round(accuracy_score(y_test, pred),3)*100))
  print('F1 Score = {}%'.format(round(f1_score(y_test, pred, average='weighted'),3)*100))
  print('Precision Score = {}%'.format(round(precision_score(y_test, pred, average='weighted'),3)*100))
  print('Recall Score = {}%'.format(round(recall_score(y_test, pred, average='weighted'),3)*100))
  print('\n \033[1mConfusiton Matrix:\033[0m\n',confusion_matrix(y_test, pred))
  print('\n\033[1mClassification Report:\033[0m\n',classification_report(y_test, pred))

  auc_roc(y_test, pred_prob, curves=['each_class'])
  plt.show()

# Visualizing Function
  def auc_roc_plot(y_test,pred):
    ref = [0 for _ in range(len(y_test))]
    ref_auc = roc_auc_score(y_test,ref)
    lr_auc = roc_auc_score(y_test, pred)

    ns_fpr, ns_tpr, _ = roc_curve(y_test,ref)
    lr_fpr, lr_tpr, _ = roc_curve(y_test,pred)

    plt.plot(ns_fpr, ns_tpr, linestyle='=')
    plt.plot(lr_fpr, lr_tpr, marker='*', label='AUC = {}'.format(round(roc_auc_score(y_test,pred)*100,2)))
    plt.xlabel('Flase Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()

"""## 1. Logistic Regression"""

# Building Logistic Regression Classifier

log_reg_model = LogisticRegression()
log_reg_model.fit(X_train, y_train)
pred = log_reg_model.predict(X_test)
pred_prob = log_reg_model.predict_proba(X_test)
classification_summary(pred,pred_prob, y_test, 'Logistic Regression (LR)')

"""## 2. Decision Tree Classifier"""

# Building Decision Tree Classifier

DT_model = DecisionTreeClassifier()
DT_model.fit(X_train, y_train)
pred = DT_model.predict(X_test)
pred_prob = DT_model.predict_proba(X_test)
classification_summary(pred,pred_prob, y_test, 'Decision Tree Classifier (DT)')

"""## 3. Random Forest Classifier:"""

# Building Forrest Classifier

RF_model = RandomForestClassifier()
RF_model.fit(X_train,y_train)
pred = RF_model.predict(X_test)
pred_prob = RF_model.predict_proba(X_test)
classification_summary(pred, pred_prob, y_test, 'Random Forrest Classifier (RF)')

"""## 4. Naive Bayes Classifier"""

# Building Naive Bayes Classifier

NB_model = BernoulliNB()
NB_model.fit(X_train,y_train)
pred = NB_model.predict(X_test)
pred_prob = NB_model.predict_proba(X_test)
classification_summary(pred,pred_prob, y_test,'Naive Bayes Classifier (NB')

"""## Comparing all the models"""

# Plotting Confusion-Matrix of all predictive Models

labels=['Positive','Negative']
def plot_cm(y_true, y_pred):
  cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
  cm_sum = np.sum(cm, axis=1, keepdims=True)
  cm_prec = cm / cm_sum.astype(float) * 100
  annot = np.empty_like(cm).astype(str)
  nrows, ncols = cm.shape
  for i in range(nrows):
    for j in range(ncols):
      c = cm[i,j]
      p = cm_prec[i,j]
      if i==j:
        s = cm_sum[i]
        annot[i,j] = '%.1f%%\n%d/%d' % (p,c,s)
      elif c==0:
        annot[i,j] = ''
      else:
        annot[i,j] = '%.1f%%\n%d' % (p,c)
  cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
  cm.columns=labels
  cm.index=labels
  cm.index.name = 'Actual'
  cm.columns.name = 'Predicted'
  sns.heatmap(cm, annot=annot, fmt='')

def conf_mat_plot(all_models):
  plt.figure(figsize=[20,3*math.ceil(1+len([all_models])/4)])

  for i in range(len(all_models)):
    if len(labels)<=4:
      plt.subplot(1,4,i+1)
    else:
      plt.subplot(math.ceil(len(all_models)/2),2,i+1)
    pred = all_models[i].predict(X_test)
    sns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.0f')
    plt.title(result_df.index[i])
    plt.yticks([0,1,2],labels=['Predicted Negative','Predicted Netural','Predicted Positive'],rotation=45)
    plt.xticks([0,1,2],labels=['Actual Negative','Actual Netural','Actual Positive'],rotation=45)
  plt.tight_layout()
  plt.show()

conf_mat_plot([log_reg_model, DT_model, RF_model, NB_model])

# Camparing all the models score

plt.figure(figsize=[12,5])
sns.heatmap(result_df, annot=True, vmin=40, vmax=100.0, cmap='RdYlBu', fmt='.1f')
plt.show()

"""In evaluating there are a  few  metrics we can use to measure performance:

* **Accuracy:** The ratio of he number of correct predictions and te al number of predictions.
* **Precision:** The number of ue positives over the number of predicted positives.
* **Recall:** The number of true positives over the total number of actual positives.
* **F1-Score:** The harmonic mean of precision and recall.

Since F1-Score is the combined idea between **Precision** and **Recall** then I will focus on the F1-Score of the Random Forest classifier since it performs best on the current dataset.  Performance can be incresed using hyperparametr tuning.  Let's try some random values for some of the parameters.
"""

param_grid = {
    'n_estimators': [100,200,300,400],
    'max_features': ['sqrt','log2'],
    'max_depth': [1,2,4,5,6,7,8],
    'criterion': ['gini','entropy']
}

CV_rfc = GridSearchCV(estimator=RF_model, param_grid=param_grid, cv=5,n_jobs=-1)
CV_rfc.fit(X_train,y_train)

CV_rfc.best_params_

RF_model_cv=RandomForestClassifier(random_state=42,**CV_rfc.best_params_)
RF_model_cv.fit(X_train,y_train)

conf_mat_plot([RF_model_cv])

pred = RF_model_cv.predict(X_test)
pred_prob = RF_model_cv.predict_proba(X_test)
print(classification_report(y_test,pred))
accuracy_score(y_test,pred)
# print(round(roc_auc_score(y_test, pred_prob, multi_class='ovr'),3)*100)
auc_roc(y_test,pred_prob)

"""# Conclusions

**From the above experimentation we can conclude that:**
* The Dataset consists of 1.6M record but we split the data by a factor of 1000 for quick preprocessing/debugging.
* Basic preprocessing techniques help us to get rid of unwanted character and gave us the clean data.
* The labels in the target variable were somewhat uniformally distributed.
* Testing multiple algorithms with default settings .
* The performace of models were almost similar.
* Considering the all metrics, we see Random Forest Classifier performed the best on the current dataset.
* Being an equal contendor,it is wise to also consider simpler models like 
* Logisitic Regression as it is more generalisable & computationally less expensive.
* We tried to apply hyperparameter tuning on RandomForest but that let to degrade the performace, so one can say we can consider the random forest model with default settings.
"""